{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB GenAI - LLMs - OpenAI GPT API Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a simple chatbot that can answer basic questions about a given topic (e.g., history, technology).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set OpenAI API key (Replace with your paid API key)\n",
    "openai.api_key = \"openai_api_key\"\n",
    "\n",
    "# Chatbot function with adjustable parameters\n",
    "def chatbot(prompt, temperature=0.7, max_tokens=100, top_p=1.0, frequency_penalty=0, presence_penalty=0, n=1, stop=None):\n",
    "    client = openai.OpenAI(api_key=openai.api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Change to \"gpt-4\" if needed\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        n=n,\n",
    "        stop=stop\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example chat\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    print(\"Chatbot:\", chatbot(user_input, temperature=0.7, max_tokens=50, top_p=0.9, frequency_penalty=0.5, presence_penalty=0.6, n=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature - Controls randomness. Higher values (e.g., 0.9) make responses more creative, lower values (0.1) make them more predictable.\n",
    "max_tokens - Limits response length. Lower values (50) generate shorter responses.\n",
    "top_p - Controls probability distribution. Lower values (0.5) make the model pick safer responses.\n",
    "frequency_penalty - Reduces repetition of words. Higher values (1.5) discourage repeated phrases.\n",
    "presence_penalty - Encourages new topics. Higher values (1.5) make responses introduce new ideas.\n",
    "n - Number of responses generated. Setting n=3 will return 3 different responses.\n",
    "stop - Defines words that stop the response generation early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Write a script that takes a long text input and summarizes it into a few sentences.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set OpenAI API key (Replace with your paid API key)\n",
    "openai.api_key = \"openai_api_key\"\n",
    "\n",
    "# Summarization function with adjustable parameters\n",
    "def summarize_text(text, temperature=0.5, max_tokens=100, top_p=1.0, frequency_penalty=0, presence_penalty=0, best_of=1, logprobs=None):\n",
    "    client = openai.OpenAI(api_key=openai.api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Change to \"gpt-4\" if needed\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Summarize this text in a few sentences: {text}\"}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        best_of=best_of\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example long text\n",
    "long_text = \"\"\"\n",
    "Artificial Intelligence (AI) is a field of computer science that focuses on creating intelligent machines capable of performing tasks that typically require human intelligence. \n",
    "These tasks include learning, reasoning, problem-solving, perception, and natural language processing. AI is categorized into narrow AI, which is designed for specific tasks, \n",
    "and general AI, which aims to perform any cognitive task a human can do. AI is widely used in industries such as healthcare, finance, education, and entertainment, \n",
    "enhancing efficiency and decision-making.\n",
    "\"\"\"\n",
    "\n",
    "# Summarize the text\n",
    "summary = summarize_text(long_text, temperature=0.5, max_tokens=50, top_p=0.9, frequency_penalty=0.3, presence_penalty=0.2, best_of=2)\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature - Higher values (0.9) make summaries more creative, lower values (0.1) make them more factual.\n",
    "max_tokens - Limits the summary length. Lower values (50) make summaries shorter.\n",
    "top_p - Controls probability sampling. Lower values (0.5) make the model pick safer responses.\n",
    "frequency_penalty - Reduces repetition. Higher values (1.5) discourage repeated words.\n",
    "presence_penalty - Encourages new ideas. Higher values (1.5) make summaries introduce new content.\n",
    "best_of - Generates multiple responses and returns the best one. Higher values (3) improve quality but use more tokens.\n",
    "logprobs - Returns log probabilities of token choices (used for debugging, typically set to None)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Develop a tool that translates text from one language to another using the API.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set OpenAI API key (Replace with your paid API key)\n",
    "openai.api_key = \"openai_api_key\"\n",
    "\n",
    "# Translation function with adjustable parameters\n",
    "def translate_text(text, target_language=\"Spanish\", temperature=0.5, max_tokens=100, top_p=1.0, frequency_penalty=0, presence_penalty=0, echo=False, logit_bias=None):\n",
    "    client = openai.OpenAI(api_key=openai.api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Change to \"gpt-4\" if needed\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Translate this text to {target_language}: {text}\"}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example text to translate\n",
    "text = \"Artificial Intelligence is revolutionizing many industries by improving efficiency and decision-making.\"\n",
    "\n",
    "# Translate to Spanish\n",
    "translation = translate_text(text, target_language=\"Spanish\", temperature=0.5, max_tokens=50, top_p=0.9, frequency_penalty=0.2, presence_penalty=0.3)\n",
    "print(\"Translated Text:\", translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature - Higher values (0.9) make translations more creative, lower values (0.1) make them more accurate.\n",
    "max_tokens - Limits the translation length. Lower values (50) shorten translations.\n",
    "top_p - Controls probability sampling. Lower values (0.5) make translations more predictable.\n",
    "frequency_penalty - Reduces repetition. Higher values (1.5) discourage repeated words.\n",
    "presence_penalty - Encourages new phrasing. Higher values (1.5) make translations more diverse.\n",
    "echo - If True, includes the original text in the response.\n",
    "logit_bias - Adjusts likelihood of specific words appearing in the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool that determines the sentiment of a given text (positive, negative, neutral).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set OpenAI API key (Replace with your paid API key)\n",
    "openai.api_key = \"openai_api_key\"\n",
    "\n",
    "# Sentiment analysis function with adjustable parameters\n",
    "def analyze_sentiment(text, temperature=0.5, max_tokens=50, top_p=1.0, frequency_penalty=0, presence_penalty=0, n=1, logprobs=None):\n",
    "    client = openai.OpenAI(api_key=openai.api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Change to \"gpt-4\" if needed\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Analyze the sentiment of this text and classify it as Positive, Negative, or Neutral: {text}\"}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        n=n\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example text for sentiment analysis\n",
    "text = \"I love this product! It works perfectly and exceeded my expectations.\"\n",
    "\n",
    "# Analyze sentiment\n",
    "sentiment = analyze_sentiment(text, temperature=0.3, max_tokens=20, top_p=0.9, frequency_penalty=0.2, presence_penalty=0.3, n=1)\n",
    "print(\"Sentiment:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature - Higher values (0.9) make sentiment analysis more creative, lower values (0.1) make it more factual.\n",
    "max_tokens - Limits response length. Lower values (20) keep it concise.\n",
    "top_p - Controls probability sampling. Lower values (0.5) make it more predictable.\n",
    "frequency_penalty - Reduces repetition. Higher values (1.5) discourage repeated words.\n",
    "presence_penalty - Encourages varied responses. Higher values (1.5) make it explore different classifications.\n",
    "n - Generates multiple responses and returns n results.\n",
    "logprobs - If set, returns log probabilities of token choices (used for debugging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Create a text completion application that generates text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set OpenAI API key (Replace with your paid API key)\n",
    "openai.api_key = \"openai_api_key\"\n",
    "\n",
    "# Text completion function with adjustable parameters\n",
    "def complete_text(prompt, temperature=0.7, max_tokens=100, top_p=1.0, frequency_penalty=0, presence_penalty=0, stop=None, best_of=1):\n",
    "    client = openai.OpenAI(api_key=openai.api_key)\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Change to \"gpt-4\" if needed\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        stop=stop,\n",
    "        best_of=best_of\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Once upon a time in a distant land, there lived a wise old man who\"\n",
    "\n",
    "# Generate text completion\n",
    "completion = complete_text(prompt, temperature=0.7, max_tokens=50, top_p=0.9, frequency_penalty=0.3, presence_penalty=0.3, stop=[\".\"], best_of=2)\n",
    "print(\"Generated Text:\", completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature - Higher values (0.9) make text more random, lower values (0.1) make it more predictable.\n",
    "max_tokens - Limits the length of the generated text. Lower values (50) keep it short.\n",
    "top_p - Controls probability sampling. Lower values (0.5) make responses safer.\n",
    "frequency_penalty - Reduces repetition. Higher values (1.5) discourage repeated phrases.\n",
    "presence_penalty - Encourages new ideas. Higher values (1.5) make text more unique.\n",
    "stop - Specifies stop words to end the response early (e.g., [\".\"] stops at the first sentence).\n",
    "best_of - Generates multiple completions and picks the best one. Higher values (3) improve quality but cost more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Google Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a basic chatbot using Google Vertex AI to answer questions about a given topic.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Function to create a basic chatbot using Vertex AI\n",
    "def vertex_ai_chatbot(prompt, temperature=0.7, max_output_tokens=100, top_p=1.0, frequency_penalty=0, presence_penalty=0, n=1, stop=None):\n",
    "    aiplatform.init(project=\"your-gcp-project-id\", location=\"us-central1\")\n",
    "\n",
    "    response = aiplatform.generation.predict(\n",
    "        model=\"chat-bison\",  # Change model if needed\n",
    "        instances=[{\"prompt\": prompt}],\n",
    "        parameters={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_output_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            \"frequency_penalty\": frequency_penalty,\n",
    "            \"presence_penalty\": presence_penalty,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response.predictions[0][\"content\"]\n",
    "\n",
    "# Example chatbot conversation\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    print(\"Chatbot:\", vertex_ai_chatbot(user_input, temperature=0.7, max_output_tokens=50, top_p=0.9, frequency_penalty=0.3, presence_penalty=0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature - Higher values (0.9) make responses more random, lower values (0.1) make them more predictable.\n",
    "max_output_tokens - Limits response length. Lower values (50) generate shorter responses.\n",
    "top_p - Controls probability sampling. Lower values (0.5) make the model pick safer responses.\n",
    "frequency_penalty - Reduces repetition. Higher values (1.5) discourage repeated phrases.\n",
    "presence_penalty - Encourages new topics. Higher values (1.5) make responses introduce new ideas.\n",
    "n - Number of responses generated. Setting n=3 will return 3 different responses.\n",
    "stop - Defines stop words that end the response early (e.g., [\".\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Develop a script that summarizes long text inputs using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=\"your-gcp-project-id\", location=\"us-central1\")\n",
    "\n",
    "# Function to summarize text using Vertex AI\n",
    "def summarize_text(text, temperature=0.5, max_output_tokens=100, top_p=1.0, frequency_penalty=0, presence_penalty=0, best_of=1, logprobs=None):\n",
    "    response = aiplatform.generation.predict(\n",
    "        model=\"text-bison\",  # Change model if needed\n",
    "        instances=[{\"prompt\": f\"Summarize this text in a few sentences: {text}\"}],\n",
    "        parameters={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_output_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            \"frequency_penalty\": frequency_penalty,\n",
    "            \"presence_penalty\": presence_penalty,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response.predictions[0][\"content\"]\n",
    "\n",
    "# Example long text\n",
    "long_text = \"\"\"\n",
    "Artificial Intelligence (AI) is a field of computer science that focuses on creating intelligent machines capable of performing tasks that typically require human intelligence. \n",
    "These tasks include learning, reasoning, problem-solving, perception, and natural language processing. AI is categorized into narrow AI, which is designed for specific tasks, \n",
    "and general AI, which aims to perform any cognitive task a human can do. AI is widely used in industries such as healthcare, finance, education, and entertainment, \n",
    "enhancing efficiency and decision-making.\n",
    "\"\"\"\n",
    "\n",
    "# Summarize the text\n",
    "summary = summarize_text(long_text, temperature=0.5, max_output_tokens=50, top_p=0.9, frequency_penalty=0.3, presence_penalty=0.2)\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature - Higher values (0.9) make summaries more creative, lower values (0.1) make them more factual.\n",
    "max_output_tokens - Limits the summary length. Lower values (50) generate shorter summaries.\n",
    "top_p - Controls probability sampling. Lower values (0.5) make the model pick safer responses.\n",
    "frequency_penalty - Reduces repetition. Higher values (1.5) discourage repeated phrases.\n",
    "presence_penalty - Encourages new topics. Higher values (1.5) make summaries introduce different ideas.\n",
    "best_of - Generates multiple responses and returns the best one. Higher values (3) improve quality but use more tokens.\n",
    "logprobs - If set, returns log probabilities of token choices (used for debugging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Create a tool that translates text from one language to another using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=\"your-gcp-project-id\", location=\"us-central1\")\n",
    "\n",
    "# Function to translate text using Vertex AI\n",
    "def translate_text(text, target_language=\"Spanish\", temperature=0.7, max_output_tokens=100, top_p=1.0, frequency_penalty=0, presence_penalty=0, echo=False, logit_bias=None):\n",
    "    response = aiplatform.generation.predict(\n",
    "        model=\"text-bison\",  # Change model if needed\n",
    "        instances=[{\"prompt\": f\"Translate this text to {target_language}: {text}\"}],\n",
    "        parameters={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_output_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            \"frequency_penalty\": frequency_penalty,\n",
    "            \"presence_penalty\": presence_penalty,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response.predictions[0][\"content\"]\n",
    "\n",
    "# Example text for translation\n",
    "text = \"Artificial Intelligence is transforming industries by enhancing automation and efficiency.\"\n",
    "\n",
    "# Translate to Spanish\n",
    "translated_text = translate_text(text, target_language=\"Spanish\", temperature=0.5, max_output_tokens=50, top_p=0.9, frequency_penalty=0.2, presence_penalty=0.3)\n",
    "print(\"Translated Text:\", translated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature - Higher values (0.9) make translations more creative, lower values (0.1) make them more accurate.\n",
    "max_output_tokens - Limits translation length. Lower values (50) shorten the translation.\n",
    "top_p - Controls probability sampling. Lower values (0.5) make translations more predictable.\n",
    "frequency_penalty - Reduces repetition. Higher values (1.5) discourage repeated words.\n",
    "presence_penalty - Encourages varied responses. Higher values (1.5) make translations more diverse.\n",
    "echo - If True, includes the original text in the response.\n",
    "logit_bias - Adjusts likelihood of specific words appearing in the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool using Google Vertex AI to determine the sentiment of a given text.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=\"your-gcp-project-id\", location=\"us-central1\")\n",
    "\n",
    "# Function to analyze sentiment using Vertex AI\n",
    "def analyze_sentiment(text, temperature=0.5, max_output_tokens=50, top_p=1.0, frequency_penalty=0, presence_penalty=0, n=1, logprobs=None):\n",
    "    response = aiplatform.generation.predict(\n",
    "        model=\"text-bison\",  # Change model if needed\n",
    "        instances=[{\"prompt\": f\"Analyze the sentiment of this text. Classify as Positive, Negative, or Neutral: {text}\"}],\n",
    "        parameters={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_output_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            \"frequency_penalty\": frequency_penalty,\n",
    "            \"presence_penalty\": presence_penalty,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response.predictions[0][\"content\"]\n",
    "\n",
    "# Example text for sentiment analysis\n",
    "text = \"I love this product! It works perfectly and exceeded my expectations.\"\n",
    "\n",
    "# Analyze sentiment\n",
    "sentiment = analyze_sentiment(text, temperature=0.3, max_output_tokens=20, top_p=0.9, frequency_penalty=0.2, presence_penalty=0.3, n=1)\n",
    "print(\"Sentiment:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature - Higher values (0.9) make sentiment analysis more creative, lower values (0.1) make it more factual.\n",
    "max_output_tokens - Limits response length. Lower values (20) keep it concise.\n",
    "top_p - Controls probability sampling. Lower values (0.5) make it more predictable.\n",
    "frequency_penalty - Reduces repetition. Higher values (1.5) discourage repeated words.\n",
    "presence_penalty - Encourages varied responses. Higher values (1.5) make it explore different classifications.\n",
    "n - Generates multiple responses and returns n results.\n",
    "logprobs - If set, returns log probabilities of token choices (used for debugging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Develop a text completion application using Google Vertex AI to generate text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=\"your-gcp-project-id\", location=\"us-central1\")\n",
    "\n",
    "# Function to generate text completion using Vertex AI\n",
    "def complete_text(prompt, temperature=0.7, max_output_tokens=100, top_p=1.0, frequency_penalty=0, presence_penalty=0, stop=None, best_of=1):\n",
    "    response = aiplatform.generation.predict(\n",
    "        model=\"text-bison\",  # Change model if needed\n",
    "        instances=[{\"prompt\": prompt}],\n",
    "        parameters={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_output_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            \"frequency_penalty\": frequency_penalty,\n",
    "            \"presence_penalty\": presence_penalty,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response.predictions[0][\"content\"]\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Once upon a time in a distant land, there lived a wise old man who\"\n",
    "\n",
    "# Generate text completion\n",
    "completion = complete_text(prompt, temperature=0.7, max_output_tokens=50, top_p=0.9, frequency_penalty=0.3, presence_penalty=0.3, stop=[\".\"], best_of=2)\n",
    "print(\"Generated Text:\", completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature - Higher values (0.9) make text more random, lower values (0.1) make it more predictable.\n",
    "max_output_tokens - Limits the length of the generated text. Lower values (50) keep it short.\n",
    "top_p - Controls probability sampling. Lower values (0.5) make responses safer.\n",
    "frequency_penalty - Reduces repetition. Higher values (1.5) discourage repeated phrases.\n",
    "presence_penalty - Encourages new ideas. Higher values (1.5) make text more unique.\n",
    "stop - Specifies stop words to end the response early (e.g., [\".\"]).\n",
    "best_of - Generates multiple completions and picks the best one. Higher values (3) improve quality but cost more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
